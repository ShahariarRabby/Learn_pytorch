{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear Regression allows us to underestand relationship between two contimous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of simple liner Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22485659fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n = 50\n",
    "x = np.random.randn(n)\n",
    "y = x * np.random.randn(n)\n",
    "\n",
    "colors = np.random.randn(n)\n",
    "plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1)) (np.unique(x)))\n",
    "\n",
    "plt.scatter(x, y, c = colors, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a liner regression model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_value = [i for i in range(11)]\n",
    "x_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "x_train = np.array(x_value, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = 2x + 1\n",
    "y_value = [2 * i + 1 for i in x_value]\n",
    "y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_value, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "1. Liner model\n",
    "    * True eqn : y = 2x + 1\n",
    "2. Forward\n",
    "    * Example\n",
    "        * input x = 1\n",
    "        * y = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim, output_dim = 1, 1\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSE Loss = Mean Squered Error\n",
    "\n",
    "criteration = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer Class\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epocs 1 loss 64.36959838867188\n",
      "Epocs 2 loss 63.285911560058594\n",
      "Epocs 3 loss 62.2116813659668\n",
      "Epocs 4 loss 61.147056579589844\n",
      "Epocs 5 loss 60.092185974121094\n",
      "Epocs 6 loss 59.0472297668457\n",
      "Epocs 7 loss 58.012325286865234\n",
      "Epocs 8 loss 56.987606048583984\n",
      "Epocs 9 loss 55.97321701049805\n",
      "Epocs 10 loss 54.969276428222656\n",
      "Epocs 11 loss 53.975894927978516\n",
      "Epocs 12 loss 52.99319839477539\n",
      "Epocs 13 loss 52.02128601074219\n",
      "Epocs 14 loss 51.06025314331055\n",
      "Epocs 15 loss 50.11019515991211\n",
      "Epocs 16 loss 49.17119216918945\n",
      "Epocs 17 loss 48.24332046508789\n",
      "Epocs 18 loss 47.32664489746094\n",
      "Epocs 19 loss 46.42123031616211\n",
      "Epocs 20 loss 45.5271110534668\n",
      "Epocs 21 loss 44.644351959228516\n",
      "Epocs 22 loss 43.77298355102539\n",
      "Epocs 23 loss 42.913028717041016\n",
      "Epocs 24 loss 42.064517974853516\n",
      "Epocs 25 loss 41.22746276855469\n",
      "Epocs 26 loss 40.40187072753906\n",
      "Epocs 27 loss 39.587745666503906\n",
      "Epocs 28 loss 38.78507614135742\n",
      "Epocs 29 loss 37.99385070800781\n",
      "Epocs 30 loss 37.214054107666016\n",
      "Epocs 31 loss 36.44566345214844\n",
      "Epocs 32 loss 35.688636779785156\n",
      "Epocs 33 loss 34.94294738769531\n",
      "Epocs 34 loss 34.208560943603516\n",
      "Epocs 35 loss 33.485416412353516\n",
      "Epocs 36 loss 32.77346420288086\n",
      "Epocs 37 loss 32.07266616821289\n",
      "Epocs 38 loss 31.382946014404297\n",
      "Epocs 39 loss 30.70423126220703\n",
      "Epocs 40 loss 30.036476135253906\n",
      "Epocs 41 loss 29.37959098815918\n",
      "Epocs 42 loss 28.733509063720703\n",
      "Epocs 43 loss 28.098155975341797\n",
      "Epocs 44 loss 27.47344398498535\n",
      "Epocs 45 loss 26.859283447265625\n",
      "Epocs 46 loss 26.255598068237305\n",
      "Epocs 47 loss 25.66229248046875\n",
      "Epocs 48 loss 25.07927703857422\n",
      "Epocs 49 loss 24.506452560424805\n",
      "Epocs 50 loss 23.94373321533203\n",
      "Epocs 51 loss 23.391016006469727\n",
      "Epocs 52 loss 22.848203659057617\n",
      "Epocs 53 loss 22.315187454223633\n",
      "Epocs 54 loss 21.791881561279297\n",
      "Epocs 55 loss 21.278169631958008\n",
      "Epocs 56 loss 20.773950576782227\n",
      "Epocs 57 loss 20.27912139892578\n",
      "Epocs 58 loss 19.7935733795166\n",
      "Epocs 59 loss 19.317197799682617\n",
      "Epocs 60 loss 18.849895477294922\n",
      "Epocs 61 loss 18.39154815673828\n",
      "Epocs 62 loss 17.942052841186523\n",
      "Epocs 63 loss 17.501298904418945\n",
      "Epocs 64 loss 17.069177627563477\n",
      "Epocs 65 loss 16.645580291748047\n",
      "Epocs 66 loss 16.230398178100586\n",
      "Epocs 67 loss 15.823516845703125\n",
      "Epocs 68 loss 15.424825668334961\n",
      "Epocs 69 loss 15.034218788146973\n",
      "Epocs 70 loss 14.651586532592773\n",
      "Epocs 71 loss 14.27681827545166\n",
      "Epocs 72 loss 13.909799575805664\n",
      "Epocs 73 loss 13.550423622131348\n",
      "Epocs 74 loss 13.198583602905273\n",
      "Epocs 75 loss 12.854170799255371\n",
      "Epocs 76 loss 12.517070770263672\n",
      "Epocs 77 loss 12.187179565429688\n",
      "Epocs 78 loss 11.864388465881348\n",
      "Epocs 79 loss 11.548592567443848\n",
      "Epocs 80 loss 11.239679336547852\n",
      "Epocs 81 loss 10.937549591064453\n",
      "Epocs 82 loss 10.64208984375\n",
      "Epocs 83 loss 10.35319995880127\n",
      "Epocs 84 loss 10.070768356323242\n",
      "Epocs 85 loss 9.794695854187012\n",
      "Epocs 86 loss 9.524880409240723\n",
      "Epocs 87 loss 9.261211395263672\n",
      "Epocs 88 loss 9.003594398498535\n",
      "Epocs 89 loss 8.751925468444824\n",
      "Epocs 90 loss 8.506104469299316\n",
      "Epocs 91 loss 8.266029357910156\n",
      "Epocs 92 loss 8.031601905822754\n",
      "Epocs 93 loss 7.802721977233887\n",
      "Epocs 94 loss 7.579297065734863\n",
      "Epocs 95 loss 7.361224174499512\n",
      "Epocs 96 loss 7.148407459259033\n",
      "Epocs 97 loss 6.9407572746276855\n",
      "Epocs 98 loss 6.73817253112793\n",
      "Epocs 99 loss 6.540563583374023\n",
      "Epocs 100 loss 6.347836494445801\n",
      "Cpu Time 1.3443241119384766\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    # Convert Numpy array to torch\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "    \n",
    "    # Clear Gradeients WRT\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward to get output \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = criteration(outputs, labels)\n",
    "    \n",
    "    # Getting Gradients w.r.t param\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating Parm\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epocs', epoch, 'loss', loss.data[0])\n",
    "    \n",
    "print('Cpu Time', time.time() - a)\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.73339951],\n",
       "       [  2.35265732],\n",
       "       [  3.97191477],\n",
       "       [  5.5911727 ],\n",
       "       [  7.21043015],\n",
       "       [  8.82968807],\n",
       "       [ 10.448946  ],\n",
       "       [ 12.06820297],\n",
       "       [ 13.6874609 ],\n",
       "       [ 15.30671883],\n",
       "       [ 16.9259758 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare data\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.],\n",
       "       [  3.],\n",
       "       [  5.],\n",
       "       [  7.],\n",
       "       [  9.],\n",
       "       [ 11.],\n",
       "       [ 13.],\n",
       "       [ 15.],\n",
       "       [ 17.],\n",
       "       [ 19.],\n",
       "       [ 21.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '03 Linear Regression with PyTorch.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"03 Linear Regression with PyTorch.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create class\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "input_dim, output_dim = 1, 1\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Available\n"
     ]
    }
   ],
   "source": [
    "# Everithing on the CPU is same as GPU\n",
    "# But there is some change Let's see\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda Available')\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSE Loss = Mean Squered Error\n",
    "\n",
    "criteration = nn.MSELoss()\n",
    "\n",
    "# Optimizer Class\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Available\n",
      "Epocs 1 loss 156.33799743652344\n",
      "Epocs 2 loss 154.64662170410156\n",
      "Epocs 3 loss 152.96469116210938\n",
      "Epocs 4 loss 151.29234313964844\n",
      "Epocs 5 loss 149.6297607421875\n",
      "Epocs 6 loss 147.97711181640625\n",
      "Epocs 7 loss 146.3345184326172\n",
      "Epocs 8 loss 144.7021026611328\n",
      "Epocs 9 loss 143.0800323486328\n",
      "Epocs 10 loss 141.4684600830078\n",
      "Epocs 11 loss 139.86749267578125\n",
      "Epocs 12 loss 138.27725219726562\n",
      "Epocs 13 loss 136.69786071777344\n",
      "Epocs 14 loss 135.12940979003906\n",
      "Epocs 15 loss 133.57205200195312\n",
      "Epocs 16 loss 132.02586364746094\n",
      "Epocs 17 loss 130.49093627929688\n",
      "Epocs 18 loss 128.9673614501953\n",
      "Epocs 19 loss 127.45520782470703\n",
      "Epocs 20 loss 125.95458984375\n",
      "Epocs 21 loss 124.46553039550781\n",
      "Epocs 22 loss 122.98812866210938\n",
      "Epocs 23 loss 121.52240753173828\n",
      "Epocs 24 loss 120.0684585571289\n",
      "Epocs 25 loss 118.62628936767578\n",
      "Epocs 26 loss 117.19595336914062\n",
      "Epocs 27 loss 115.77749633789062\n",
      "Epocs 28 loss 114.37094116210938\n",
      "Epocs 29 loss 112.9762954711914\n",
      "Epocs 30 loss 111.59359741210938\n",
      "Epocs 31 loss 110.22283172607422\n",
      "Epocs 32 loss 108.864013671875\n",
      "Epocs 33 loss 107.51718139648438\n",
      "Epocs 34 loss 106.18229675292969\n",
      "Epocs 35 loss 104.8593521118164\n",
      "Epocs 36 loss 103.54834747314453\n",
      "Epocs 37 loss 102.249267578125\n",
      "Epocs 38 loss 100.96208953857422\n",
      "Epocs 39 loss 99.68681335449219\n",
      "Epocs 40 loss 98.42338562011719\n",
      "Epocs 41 loss 97.17179870605469\n",
      "Epocs 42 loss 95.93199920654297\n",
      "Epocs 43 loss 94.7039566040039\n",
      "Epocs 44 loss 93.4876480102539\n",
      "Epocs 45 loss 92.28302764892578\n",
      "Epocs 46 loss 91.09005737304688\n",
      "Epocs 47 loss 89.90868377685547\n",
      "Epocs 48 loss 88.73886108398438\n",
      "Epocs 49 loss 87.58055877685547\n",
      "Epocs 50 loss 86.43370819091797\n",
      "Epocs 51 loss 85.29824829101562\n",
      "Epocs 52 loss 84.17416381835938\n",
      "Epocs 53 loss 83.06134796142578\n",
      "Epocs 54 loss 81.95978546142578\n",
      "Epocs 55 loss 80.8694076538086\n",
      "Epocs 56 loss 79.79015350341797\n",
      "Epocs 57 loss 78.72195434570312\n",
      "Epocs 58 loss 77.66476440429688\n",
      "Epocs 59 loss 76.61851501464844\n",
      "Epocs 60 loss 75.58314514160156\n",
      "Epocs 61 loss 74.55859375\n",
      "Epocs 62 loss 73.54478454589844\n",
      "Epocs 63 loss 72.54167175292969\n",
      "Epocs 64 loss 71.5491714477539\n",
      "Epocs 65 loss 70.56723022460938\n",
      "Epocs 66 loss 69.59577178955078\n",
      "Epocs 67 loss 68.63475036621094\n",
      "Epocs 68 loss 67.68407440185547\n",
      "Epocs 69 loss 66.74368286132812\n",
      "Epocs 70 loss 65.81352233886719\n",
      "Epocs 71 loss 64.89350128173828\n",
      "Epocs 72 loss 63.98356628417969\n",
      "Epocs 73 loss 63.0836296081543\n",
      "Epocs 74 loss 62.19365310668945\n",
      "Epocs 75 loss 61.31355285644531\n",
      "Epocs 76 loss 60.4432487487793\n",
      "Epocs 77 loss 59.58269119262695\n",
      "Epocs 78 loss 58.73179626464844\n",
      "Epocs 79 loss 57.890499114990234\n",
      "Epocs 80 loss 57.0587272644043\n",
      "Epocs 81 loss 56.23640441894531\n",
      "Epocs 82 loss 55.423484802246094\n",
      "Epocs 83 loss 54.6198844909668\n",
      "Epocs 84 loss 53.825523376464844\n",
      "Epocs 85 loss 53.040340423583984\n",
      "Epocs 86 loss 52.26426696777344\n",
      "Epocs 87 loss 51.49722671508789\n",
      "Epocs 88 loss 50.73917007446289\n",
      "Epocs 89 loss 49.9900016784668\n",
      "Epocs 90 loss 49.24966812133789\n",
      "Epocs 91 loss 48.518089294433594\n",
      "Epocs 92 loss 47.79520034790039\n",
      "Epocs 93 loss 47.0809326171875\n",
      "Epocs 94 loss 46.37522506713867\n",
      "Epocs 95 loss 45.6779899597168\n",
      "Epocs 96 loss 44.989173889160156\n",
      "Epocs 97 loss 44.3087043762207\n",
      "Epocs 98 loss 43.636505126953125\n",
      "Epocs 99 loss 42.97251892089844\n",
      "Epocs 100 loss 42.31666564941406\n",
      "Gpu Time 0.531749963760376\n"
     ]
    }
   ],
   "source": [
    "a = time.time()\n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda Available')\n",
    "\n",
    "    epochs = 100\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # Convert Numpy array to torch\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "\n",
    "        # Clear Gradeients WRT\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward to get output \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate Loss\n",
    "        loss = criteration(outputs, labels)\n",
    "\n",
    "        # Getting Gradients w.r.t param\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating Parm\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epocs', epoch, 'loss', loss.data[0])\n",
    "        \n",
    "print('Gpu Time', time.time() - a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
