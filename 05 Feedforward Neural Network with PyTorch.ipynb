{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datas\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trin_dataset = datas.MNIST(root='./data',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "test_data = datas.MNIST(root='./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trin_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.sigmoied = nn.Sigmoid()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        \n",
    "        out = self.sigmoied(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "itter = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = FeedForward(input_dim, hidden_dim, output_dim).cuda()\n",
    "else:\n",
    "    model = FeedForward(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of FeedForward (\n",
      "  (fc1): Linear (784 -> 100)\n",
      "  (sigmoied): Sigmoid ()\n",
      "  (fc2): Linear (100 -> 10)\n",
      ")>\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters)\n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itteration  500 accracy 86.22 loss 0.7404002547264099\n",
      "itteration  1000 accracy 89.49 loss 0.3774128258228302\n",
      "itteration  1500 accracy 90.62 loss 0.38821038603782654\n",
      "itteration  2000 accracy 91.29 loss 0.2733241319656372\n",
      "itteration  2500 accracy 91.7 loss 0.4501672685146332\n",
      "itteration  3000 accracy 91.94 loss 0.22734560072422028\n",
      "itteration  3500 accracy 92.39 loss 0.3766856789588928\n",
      "itteration  4000 accracy 92.61 loss 0.24867872893810272\n",
      "itteration  4500 accracy 92.93 loss 0.2621428370475769\n",
      "itteration  5000 accracy 93.28 loss 0.2908570170402527\n",
      "itteration  5500 accracy 93.44 loss 0.14289821684360504\n",
      "itteration  6000 accracy 93.7 loss 0.31799912452697754\n",
      "itteration  6500 accracy 93.82 loss 0.09556352347135544\n",
      "itteration  7000 accracy 94.0 loss 0.1671162247657776\n",
      "itteration  7500 accracy 94.02 loss 0.16627316176891327\n",
      "itteration  8000 accracy 94.21 loss 0.35028672218322754\n",
      "itteration  8500 accracy 94.4 loss 0.14355236291885376\n",
      "itteration  9000 accracy 94.52 loss 0.1721993237733841\n",
      "itteration  9500 accracy 94.63 loss 0.18212589621543884\n",
      "itteration  10000 accracy 94.84 loss 0.16719917953014374\n",
      "itteration  10500 accracy 94.94 loss 0.219807967543602\n",
      "itteration  11000 accracy 95.09 loss 0.19425517320632935\n",
      "itteration  11500 accracy 95.18 loss 0.10116259753704071\n",
      "itteration  12000 accracy 95.2 loss 0.13840220868587494\n",
      "itteration  12500 accracy 95.37 loss 0.18147991597652435\n",
      "itteration  13000 accracy 95.46 loss 0.12349312007427216\n",
      "itteration  13500 accracy 95.52 loss 0.11303312331438065\n",
      "itteration  14000 accracy 95.53 loss 0.17065124213695526\n",
      "itteration  14500 accracy 95.64 loss 0.16285745799541473\n",
      "itteration  15000 accracy 95.7 loss 0.09321907162666321\n",
      "itteration  15500 accracy 95.8 loss 0.1029786467552185\n",
      "itteration  16000 accracy 95.81 loss 0.23051133751869202\n",
      "itteration  16500 accracy 95.9 loss 0.18657992780208588\n",
      "itteration  17000 accracy 95.97 loss 0.06689253449440002\n",
      "itteration  17500 accracy 96.02 loss 0.1430635303258896\n",
      "itteration  18000 accracy 96.09 loss 0.2105148285627365\n",
      "itteration  18500 accracy 96.08 loss 0.24488535523414612\n",
      "itteration  19000 accracy 96.24 loss 0.1012275442481041\n",
      "itteration  19500 accracy 96.28 loss 0.0688004195690155\n",
      "itteration  20000 accracy 96.24 loss 0.3144057095050812\n",
      "itteration  20500 accracy 96.33 loss 0.09692897647619247\n",
      "itteration  21000 accracy 96.42 loss 0.04584949091076851\n",
      "itteration  21500 accracy 96.41 loss 0.13910961151123047\n",
      "itteration  22000 accracy 96.48 loss 0.202337384223938\n",
      "itteration  22500 accracy 96.46 loss 0.10510866343975067\n",
      "itteration  23000 accracy 96.57 loss 0.10930030792951584\n",
      "itteration  23500 accracy 96.54 loss 0.13203033804893494\n",
      "itteration  24000 accracy 96.57 loss 0.14379969239234924\n",
      "itteration  24500 accracy 96.59 loss 0.15256530046463013\n",
      "itteration  25000 accracy 96.67 loss 0.14613784849643707\n",
      "itteration  25500 accracy 96.62 loss 0.05448049679398537\n",
      "itteration  26000 accracy 96.74 loss 0.07501135021448135\n",
      "itteration  26500 accracy 96.76 loss 0.12275955826044083\n",
      "itteration  27000 accracy 96.76 loss 0.07889307290315628\n",
      "itteration  27500 accracy 96.79 loss 0.030641809105873108\n",
      "itteration  28000 accracy 96.8 loss 0.10082792490720749\n",
      "itteration  28500 accracy 96.88 loss 0.1403486430644989\n",
      "itteration  29000 accracy 96.8 loss 0.037167102098464966\n",
      "itteration  29500 accracy 96.83 loss 0.12360378354787827\n",
      "itteration  30000 accracy 96.91 loss 0.09995777904987335\n",
      "itteration  30500 accracy 96.99 loss 0.05997990444302559\n",
      "itteration  31000 accracy 96.98 loss 0.10928794741630554\n",
      "itteration  31500 accracy 96.99 loss 0.06337807327508926\n",
      "itteration  32000 accracy 96.98 loss 0.05537526309490204\n",
      "itteration  32500 accracy 96.93 loss 0.048256050795316696\n",
      "itteration  33000 accracy 97.09 loss 0.03770754486322403\n",
      "itteration  33500 accracy 97.0 loss 0.05194709450006485\n",
      "itteration  34000 accracy 96.97 loss 0.05588194355368614\n",
      "itteration  34500 accracy 97.1 loss 0.133024200797081\n",
      "itteration  35000 accracy 97.09 loss 0.039113353937864304\n",
      "itteration  35500 accracy 97.13 loss 0.09982415288686752\n",
      "itteration  36000 accracy 97.12 loss 0.028272466734051704\n",
      "itteration  36500 accracy 97.21 loss 0.04724614694714546\n",
      "itteration  37000 accracy 97.12 loss 0.06304658204317093\n",
      "itteration  37500 accracy 97.07 loss 0.045165929943323135\n",
      "itteration  38000 accracy 97.21 loss 0.047574326395988464\n",
      "itteration  38500 accracy 97.19 loss 0.12219913303852081\n",
      "itteration  39000 accracy 97.25 loss 0.032036684453487396\n",
      "itteration  39500 accracy 97.22 loss 0.0627230554819107\n",
      "itteration  40000 accracy 97.24 loss 0.027998117730021477\n",
      "itteration  40500 accracy 97.19 loss 0.026957735419273376\n",
      "itteration  41000 accracy 97.3 loss 0.06805311888456345\n",
      "itteration  41500 accracy 97.24 loss 0.13796378672122955\n",
      "itteration  42000 accracy 97.25 loss 0.06128527224063873\n",
      "itteration  42500 accracy 97.31 loss 0.05394939333200455\n",
      "itteration  43000 accracy 97.35 loss 0.023950448259711266\n",
      "itteration  43500 accracy 97.27 loss 0.057196393609046936\n",
      "itteration  44000 accracy 97.35 loss 0.17576415836811066\n",
      "itteration  44500 accracy 97.36 loss 0.027136072516441345\n",
      "itteration  45000 accracy 97.41 loss 0.12924884259700775\n",
      "itteration  45500 accracy 97.33 loss 0.06624528765678406\n",
      "itteration  46000 accracy 97.4 loss 0.0972752571105957\n",
      "itteration  46500 accracy 97.37 loss 0.08241407573223114\n",
      "itteration  47000 accracy 97.43 loss 0.038019128143787384\n",
      "itteration  47500 accracy 97.43 loss 0.04430563747882843\n",
      "itteration  48000 accracy 97.45 loss 0.024257980287075043\n",
      "itteration  48500 accracy 97.43 loss 0.04033274203538895\n",
      "itteration  49000 accracy 97.48 loss 0.04186449944972992\n",
      "itteration  49500 accracy 97.49 loss 0.02462584525346756\n",
      "itteration  50000 accracy 97.46 loss 0.034115418791770935\n",
      "itteration  50500 accracy 97.53 loss 0.061009667813777924\n",
      "itteration  51000 accracy 97.46 loss 0.04380185902118683\n",
      "itteration  51500 accracy 97.54 loss 0.04440011829137802\n",
      "itteration  52000 accracy 97.49 loss 0.06871480494737625\n",
      "itteration  52500 accracy 97.53 loss 0.11469575762748718\n",
      "itteration  53000 accracy 97.55 loss 0.04615938290953636\n",
      "itteration  53500 accracy 97.47 loss 0.09141046553850174\n",
      "itteration  54000 accracy 97.55 loss 0.0810466855764389\n",
      "itteration  54500 accracy 97.55 loss 0.03822833672165871\n",
      "itteration  55000 accracy 97.54 loss 0.04278679937124252\n",
      "itteration  55500 accracy 97.56 loss 0.0638602077960968\n",
      "itteration  56000 accracy 97.54 loss 0.06787320971488953\n",
      "itteration  56500 accracy 97.58 loss 0.03773913532495499\n",
      "itteration  57000 accracy 97.57 loss 0.13054199516773224\n",
      "itteration  57500 accracy 97.54 loss 0.0894545391201973\n",
      "itteration  58000 accracy 97.62 loss 0.05323948338627815\n",
      "itteration  58500 accracy 97.59 loss 0.019148940220475197\n",
      "itteration  59000 accracy 97.61 loss 0.02168586663901806\n",
      "itteration  59500 accracy 97.63 loss 0.04652160778641701\n",
      "itteration  60000 accracy 97.58 loss 0.031836334615945816\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(epochs):\n",
    "    for i, (images, lables) in enumerate(train_loader):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, 28*28)).cuda()\n",
    "            lables = Variable(lables).cuda()\n",
    "        else:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            lables = Variable(lables)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_fun(outputs, lables)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        itter+=1\n",
    "        \n",
    "        if itter % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for images, lables in test_loader:\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1,28*28)).cuda()\n",
    "                else:\n",
    "                    images = Variable(images.view(-1,28*28)).cuda()\n",
    "                    \n",
    "                outputs = model(images)\n",
    "                \n",
    "                _, predicts = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total += lables.size(0)\n",
    "                \n",
    "                correct += (predicts.cpu() == lables.cpu()).sum()\n",
    "            \n",
    "            accuracy = 100 * correct/ total\n",
    "            \n",
    "            print('itteration ', itter, 'accracy', accuracy, 'loss', loss.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
